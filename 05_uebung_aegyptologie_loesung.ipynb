{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lizenz: Das folgende Lehrmaterial kann unter einer [CC-BY-SA 4.0](https://creativecommons.org/licenses/by/4.0/legalcode) Lizenz frei verwendet, verbreitet und modifiziert werden._   \n",
    " _Authoren: Jana Lasser (jana.lasser@ds.mpg.de)_  \n",
    " _Das Lehrmaterial wurde im Zuge des Projektes \"Daten Lesen Lernen\", gefördert vom Stifterverband und der Heinz Nixdorf Stiftung erstellt._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lösungen zu Übung 05 - Ägyptologie: Daten finden und einlesen und mit Histogrammen visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>Inhalt\n",
    "---\n",
    "* [Daten beschaffen](#daten_beschaffen)\n",
    "* [Unicode](#unicode)\n",
    "* [Histogramme](#histogramme) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"funktionen\"></a>1. Daten beschaffen\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Git-Repository ist das öffentlich zugängliche Repository der Seite copticscriptorium.org. Auf dieser Seite werden koptische Texte annotiert und aufbereitet zur Verfügung gestellt. Das [Projekt coptic scriptorium](https://linguistics.georgetown.edu/research/grants/coptic-scriptorium/) wird von der Abteilung Linguistik der Georgetown University vorangetrieben. Es ist wichtig, sich einen Überblick über die Herkunft der Daten zu verschaffen, um ihre Qualität und Zuverlässigkeit einschätzen zu können. Ist die Herkunft der Daten gut dokumentiert? Sind Metadaten vorhanden? Gibt es wissenschaftliche Publikationen zu den Daten? All diese Fragen helfen dabei, die Güte der Daten einzuschätzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XML](https://de.wikipedia.org/wiki/Extensible_Markup_Language) ist eine Auszeichnungssprache zur Darstellung hierarchischer Daten (wie z.B. Text, der in Kapiteln, Sätzen und Wörtern organisiert und mit Kommentaren und anderen Auszeichnungen versehen ist). [TEI-XML](https://de.wikipedia.org/wiki/Text_Encoding_Initiative) setzt auf XML auf und legt einen Standard zur Beschreibung von Text fest. TEI-XML ist im Bereich der Textverarbeitung sehr weit verbreitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tei_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-98cc859d6893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# das spezielle Format versteht. Das erledigt die Bibliothek \"tei_reader\" für\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# uns, die auf TEI-XML-Dateien spezialisiert sind.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtei_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# os gibt Zugriff auf die Funktionen des Betriebssystems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tei_reader'"
     ]
    }
   ],
   "source": [
    "# 1.D\n",
    "\n",
    "# um XML-Dateien lesen zu können, benötigen wir einen sog. \"Parser\", der\n",
    "# das spezielle Format versteht. Das erledigt die Bibliothek \"tei_reader\" für\n",
    "# uns, die auf TEI-XML-Dateien spezialisiert sind.\n",
    "import tei_reader\n",
    "# os gibt Zugriff auf die Funktionen des Betriebssystems\n",
    "import os\n",
    "\n",
    "# das ist der Pfad zu dem Verzeichnis, im dem alle koptischen Texte liegen\n",
    "path_copt = \"daten/sahidica.mark_TEI/koptisch\"\n",
    "\n",
    "# mit der Funktion \"listdir()\" lassen sich alle in einem Verzeichnis enthaltenen\n",
    "# Dateien auflisten. Wir listen also \n",
    "filenames_copt = os.listdir(path_copt)\n",
    "\n",
    "# da wir die Dateien gleich in der richtigen Reihe einlesen wollen, sortieren wir\n",
    "# diese Liste noch\n",
    "filenames_copt.sort()\n",
    "\n",
    "# hier erzeugen wir ein \"TeiReader\" Objekt, das die Funktion \"read_file()\"\n",
    "# besitzt. Diesem Objekt können wir Pfade zu TEI-XML-Dateien geben, die\n",
    "# es dann für uns einliest und uns Zugriff auf den enthaltenen Text gibt\n",
    "reader = tei_reader.TeiReader()\n",
    "\n",
    "# dies ist der gesamte Pfad zur ersten XML-Datei im Verzeichnis\n",
    "path_to_file = os.path.join(path_copt, filenames_copt[0])\n",
    "\n",
    "# lies mit Hilfe der read_file()-Funktion die XML-Datei ein\n",
    "corpus = reader.read_file(path_to_file)\n",
    "\n",
    "# das Attribut \"text\" der Variable \"corpus\" beinhaltet \n",
    "# den koptischen Text, der in der XML-Datei gespeichert ist\n",
    "text_copt = corpus.text\n",
    "\n",
    "# zeige die ersten 100 Zeichen des Textes an\n",
    "print(text_copt[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.E\n",
    "# erzeuge wieder einen \"reader\"\n",
    "reader = tei_reader.TeiReader()\n",
    "\n",
    "# in dieser Liste werden wir die koptischen Kapitel speichern\n",
    "chapters_copt = []\n",
    "\n",
    "# jetzt iterieren wir über alle Dateinamen (Kapitel) im koptischen \n",
    "# Unterverzeichnis und lesen jede der 16 Dateien ein\n",
    "for fname in filenames_copt:\n",
    "    # dies ist der gesamte Pfad zur jeweiligen XML-Datei\n",
    "    path_to_file = os.path.join(path_copt, fname)\n",
    "    # lies mit Hilfe der read_file()-Funktion die XML-Datei ein\n",
    "    corpus = reader.read_file(path_to_file)\n",
    "    # das Attribut \"text\" der Variable \"corpus\" beinhaltet \n",
    "    # den koptischen Text, der in der XML-Datei gespeichert ist\n",
    "    single_text = corpus.text\n",
    "    # wir fügen den Text des geladenen Kapitels der Variablen hinzu,\n",
    "    # in der wir den gesamten koptischen Text des Markus-Evangeliums\n",
    "    # speichern\n",
    "    chapters_copt.append(single_text)\n",
    "    \n",
    "# importiere pandas und erzeuge ein DataFrame mit den Kapiteln\n",
    "import pandas as pd\n",
    "chapters_copt = pd.DataFrame({'chapter':chapters_copt})\n",
    "\n",
    "# zähle die Anzahl der Zeichen je Kapitel und füge sie in einer\n",
    "# neuen Spalte dem DataFrame hinzu\n",
    "char_count = [len(chap) for chap in chapters_copt['chapter']]\n",
    "chapters_copt['char_count'] = char_count\n",
    "chapters_copt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.E Fortsetzung\n",
    "# die Funktion iterrows() eines DataFrames liefert sowohl den Index als auch\n",
    "# die in der Zeile enthaltenen Daten zurück\n",
    "for i, row in chapters_copt.iterrows():\n",
    "    zeichen =row['char_count']\n",
    "    print(\"Kapitel {} hat {} Zeichen\".format(i+1, zeichen))\n",
    "\n",
    "gesamtanzahl = chapters_copt['char_count'].sum()\n",
    "print('Das Markusevangelium hat {} Zeichen'.format(gesamtanzahl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.F\n",
    "# das ist der Pfad zu dem Verzeichnis, im dem alle englischen Texte liegen\n",
    "path_eng = \"daten/sahidica.mark_TEI/englisch\"\n",
    "\n",
    "# alle Dateien im Verzeichnis auflisten und sortieren\n",
    "filenames_eng = os.listdir(path_eng)\n",
    "filenames_eng.sort()\n",
    "\n",
    "# dies ist der gesamte Pfad zur ersten XML-Datei im Verzeichnis\n",
    "path_to_file = os.path.join(path_eng, filenames_eng[0])\n",
    "\n",
    "# wir erzeugen ein sog. \"file-handle\", also einen \"Weg zur Datei\"\n",
    "# mit der Funktion open()\n",
    "file_handle = open(path_to_file)\n",
    "\n",
    "# wir lesen den Text mit der Funktion \"read()\" ein\n",
    "text_eng = file_handle.read()\n",
    "\n",
    "# der Englische text ist in einzelne Sätze strukturiert, die immer\n",
    "# mit einer Zahl (laufender Index) beginnen und mit einem Zeilenumbruch\n",
    "# enden. Zwischen den einzelnen Sätzen befindet sich eine Leerzeile.\n",
    "# Diese zusätzlichen Zeichen werden wir beachten müssen, wenn wir den\n",
    "# Text später automatisiert analysieren.\n",
    "print(text_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.G\n",
    "\n",
    "# in dieser Liste werden wir die englischen Kapitel speichern\n",
    "chapters_eng = []\n",
    "\n",
    "# jetzt iterieren wir über alle Dateinamen (Kapitel) im englischen \n",
    "# Unterverzeichnis und lesen jede der 16 Dateien ein\n",
    "for fname in filenames_eng:\n",
    "    # dies ist der gesamte Pfad zur jeweiligen Text-Datei\n",
    "    path_to_file = os.path.join(path_eng, fname)\n",
    "    # die Funktion open() gibt ein \"handle\" der Datei zurück, \n",
    "    # mit dem wir auf die Datei zugreifen können\n",
    "    file_handle = open(path_to_file)\n",
    "    # lies mit Hilfe der read()-Funktion die XML-Datei ein\n",
    "    single_text = file_handle.read()\n",
    "    # wir fügen den Text des geladenen Kapitels der Liste hinzu,\n",
    "    # in dem wir alle Texte der einzelnen Kapitel speichern\n",
    "    chapters_eng.append(single_text)\n",
    "\n",
    "# erzeuge ein DataFrame aus der Liste\n",
    "chapters_eng = pd.DataFrame({'chapter':chapters_eng})\n",
    "\n",
    "# zähle die Anzahl der Zeichen je Kapitel und füge sie in einer\n",
    "# neuen Spalte dem DataFrame hinzu\n",
    "char_count = [len(chap) for chap in chapters_eng['chapter']]\n",
    "chapters_eng['char_count'] = char_count\n",
    "chapters_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.H\n",
    "# und alles nochmal für die Deutschen Texte\n",
    "\n",
    "path_deu = \"daten/sahidica.mark_TEI/deutsch\"\n",
    "filenames_deu = os.listdir(path_deu)\n",
    "filenames_deu.sort()\n",
    "\n",
    "chapters_deu = []\n",
    "for fname in filenames_deu:\n",
    "    path_to_file = os.path.join(path_deu, fname)\n",
    "    file_handle = open(path_to_file)\n",
    "    single_text = file_handle.read()\n",
    "    chapters_deu.append(single_text)\n",
    "\n",
    "chapters_deu = pd.DataFrame({'chapter':chapters_deu})\n",
    "char_count = [len(chap) for chap in chapters_deu['chapter']]\n",
    "chapters_deu['char_count'] = char_count\n",
    "# gleich dem englischen Text ist der Deutsche in Sätze gegliedert,\n",
    "# die immer mit einem laufenden Index eingeleitet werden.\n",
    "chapters_deu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"unicode\"></a> 2. Unicode\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anfang](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode codiert Symbole digital, indem es die verschiedenen Symbole \"durchnummeriert\", dadurch wird es möglich, eine große Vielfalt von Symbolen zu codieren. Die große Anzahl an möglichen Zeichen ist hilfreich, um die vielfalt der Symbole darzustellen, die verschiedene Kulturen und verschiedene Schriften verwenden. Auch Emojis werden in Unicode codiert.  \n",
    "Für jede Nummer ist festgelegt, welches Symbol sie darstellt und diese Nummern werden dann von darstellenden Programmen (wie z.B. das Jupyter-Notebook oder WhatsApp) zurück in die entsprechenden Symbole übersetzt. In Unicode können 17 \"Ebenen\" codiert werden - Im Endeffekt entspricht jede Ebene einer \"Stelle\" in der Nummerierung. Ein Zeichen auf Ebene 17 würde also mit 17 Zahlen codiert werden. Im aktuellen Unicode-Standard werden 6 Ebenen verwendet und es können 137.929 Zeichen codiert werden. Die aktuell verwendeten Ebenen sind in \"Blöcke\" unterteilt, in denen sich jeweils Zeichen verschiedener schriften finden:  \n",
    "\n",
    "![Bild](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Roadmap_to_Unicode_BMP_multilingual.svg/langde-1920px-Roadmap_to_Unicode_BMP_multilingual.svg.png)\n",
    "\n",
    "\n",
    "Die von uns verwendeten koptischen Symbole liegen im Block für \"andere europäische Schriften\" (erste Zeile, blau). Das ist dadurch erkennbar, dass unsere koptischen Symbole alle mit ```u02``` oder ```u03``` beginnen, wobei das ```u``` für Unicode steht, während die Zahl den Block bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Symbole ```\"\\n\"``` (Zeilenumbruch), ```\".\"```, ```\"[\"``` und ```\"]\"``` sowie das Leerzeichen gehören nicht zum Koptischen Alphabet. Im ersten Kapitel kommen 29 koptische Schriftzeichen vor, das koptische Alphabet hat aber 30 Schriftzeichen. Das Zeichen, das im ersten Kapitel nicht vorkommt, ist ```\\u2caf``` bzw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\u2caf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exkurs (absolut freiwillig)\n",
    "# einfach herausfinden, welches Symbol im ersten Kapitel nicht\n",
    "# vorkommt lässt sich durch einen Vergleich mit den Symbolen\n",
    "# im dritten Kapitel. Dafür können wir uns die Mengen-Operation\n",
    "# der Differenz zunutze machen. Siehe Dokumentation von Mengen\n",
    "# in Python hier: https://docs.python.org/2/library/sets.html\n",
    "\n",
    "# erstelle die Menge der Symbole im ersten Kapitel\n",
    "chapter01_symbols = set(chapters_copt.loc[0]['chapter'])\n",
    "# erstelle die Menge der Symbole im dritten Kapitel\n",
    "chapter03_symbols = set(chapters_copt.loc[2]['chapter'])\n",
    "# erstelle die Differenz der Menge (das Resultat ist selbst eine Menge)\n",
    "difference = chapter03_symbols.difference(chapter01_symbols)\n",
    "# verwandle die Menge in eine Liste\n",
    "difference = list(difference)\n",
    "# gib die Liste aus\n",
    "print(difference)\n",
    "print(difference[0].encode('raw_unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.C\n",
    "vorname = 'Jana'\n",
    "print(vorname.lower())\n",
    "\n",
    "nachname = 'lasser'\n",
    "print(nachname.upper())\n",
    "\n",
    "testsymbol = '\\u2ca3'\n",
    "print(testsymbol)\n",
    "\n",
    "print(testsymbol.upper())\n",
    "print(testsymbol.upper().encode('raw_unicode_escape'))\n",
    "# der entsprechende Großbuchstabe liegt anscheinend im \n",
    "# Unicode-Block direkt vor dem Kleinbuchstaben. Bei\n",
    "# der Überführung in Groschreibung wird also die Unicode-\n",
    "# Nummer einfach um 1 verringert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.D\n",
    "# Kapitel 3 enthält alle 30 Schriftzeichen\n",
    "symbols = set(chapters_copt.loc[2]['chapter'])\n",
    "# eine Liste der nicht-koptischen Zeichen, die bisher so vorgekommen sind\n",
    "non_coptic = ['\\n', ' ', '.', ',', ';', '[', ']']\n",
    "# entferne die nicht-koptischen Zeichen\n",
    "symbols = [sym for sym in symbols if sym not in non_coptic]\n",
    "\n",
    "# erstelle eine Tabelle mit den koptischen Symbolen und ihrer\n",
    "# Unicode-Codierung\n",
    "symbol_table = pd.DataFrame()\n",
    "symbol_table['symbol'] = symbols\n",
    "symbol_table['unicode'] = [sym.encode('raw_unicode_escape') for\\\n",
    "                    sym in symbols]\n",
    "\n",
    "# zeige die Tabelle an\n",
    "symbol_table.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anfang](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"histogramme\"></a> 3. Histogramme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.A\n",
    "# importiere das Untermodul pyplot aus der Bibliothek\n",
    "# matplotlib zum darstellen von Grafiken\n",
    "import matplotlib.pyplot as plt\n",
    "# mit diesem Kommando teilen wir dem Jupyter-Notebook mit,\n",
    "# dass es Grafiken direkt im Notebook anzeigen soll\n",
    "%matplotlib inline\n",
    "\n",
    "# erstelle ein einfaches Histogram der Kapitellängen\n",
    "plt.hist(chapters_copt['char_count']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.B\n",
    "# die Werte liegen alle in einem Bereich von 1000 bis 9000 Zeichen\n",
    "# indem wir die \"range\" entsprechend festlegen und die Anzahl der Bins\n",
    "# auf 8 setzen, liegen die einzelnen Bins zwischen \"glatten\" Vielfachen\n",
    "# von 1000. Das ist anschaulich in einfach zu interpretieren. Diese \n",
    "# Darstellung lässt z.B. direkt ablesen, dass 6 Kapitel des koptischen\n",
    "# Markus-Evangeliums zwischen 3000 und 4000 Zeichen haben.\n",
    "\n",
    "# die rwidth auf einen Wert kleiner 1.0 zu setzen ist auch eine gute Idee,\n",
    "# da es die einzelen bins in der Darstellung besser voneinander abgrenzt.\n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=8, rwidth=0.8)\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.title('Verteilung der Kapitellängen im Markusevangelium');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.C\n",
    "# wenn eine (zu) kleine Anzahl bins gewählt wird, geht viel Information\n",
    "# verloren. \n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=2, rwidth=0.8)\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.title('Verteilung der Kapitellängen im Markusevangelium');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bei einer zu großen Anzahl von bins liegt irgendwann praktisch jede Beobachtung\n",
    "# (Kapitellänge) in ihrem eigenen Bin und es wird schwer, Information über die \n",
    "# Menge der Beobachtungen zu aggregieren\n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=100, rwidth=0.8)\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.title('Verteilung der Kapitellängen im Markusevangelium');\n",
    "# eine gute Wahl für den Datensatz scheint bins=8 (wie in 1.B) zu sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.D\n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='koptisch')\n",
    "plt.hist(chapters_eng['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='englisch')\n",
    "#plt.hist(chapters_deu['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='deutsch')\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.title('Verteilung der Kapitellängen im Markusevangelium')\n",
    "plt.legend()\n",
    "\n",
    "# die englischen Kapitel scheinen tendentiell etwas länger zu sein als die koptischen\n",
    "# damit die Darstellung vergleichbar ist, muss das Argument range und bins jeweils gleich \n",
    "# gewählt sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.E (density)\n",
    "# das Argument \"density\" verändert die Darstellung der Beobachtungen auf der y-Achse. \n",
    "# Vorher war die absolute Anzahl der Beobachtungen abgebildet, mit density=True\n",
    "# wird die normierte Anzahl der beobachtungen abgebildet, die als Wahrscheinlichkeits-\n",
    "# dichte interpretiert werden kann\n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='koptisch', density=True)\n",
    "plt.hist(chapters_eng['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='englisch', density=True)\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Wahrscheinlichkeitsdichte')\n",
    "plt.title('Wahrscheinlichkeitsdichte der Kapitellängen im Markusevangelium')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.E (cumulative)\n",
    "# wenn cumulative=True, dann zeigt jeder bin nicht nur die Anzahl der Beobachtungen\n",
    "# in ebendiesem bin sondern die Anzahl in dem bin PLUS die Anzahl der Beobachtungen\n",
    "# in allen Vorangegangenen bins. Die Anzahl der Beobachtungen im letzten Bin entspricht\n",
    "# also immer der Gesamtanzahl der Beobachtungen.\n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='koptisch', cumulative=True)\n",
    "plt.hist(chapters_eng['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='englisch', cumulative=True)\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Wahrscheinlichkeitsdichte')\n",
    "plt.title('Kumulative Verteilung der Wahrscheinlichkeitsdichten der Kapitellängen im Markusevangelium')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.E (density & cumulative)\n",
    "# ist sowohl density=True als auch cumulative=True, dann lässt sich schön beobachten,\n",
    "# dass die Wahrscheinlichkeitsdichte auf 1 normiert ist, dass heißt die kumulierten \n",
    "# Wahrscheinlichkeiten, eine Beobachtung in einem Bin zu finden ist im letzten bin gleich 1\n",
    "plt.hist(chapters_copt['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='koptisch', cumulative=True, density=True)\n",
    "plt.hist(chapters_eng['char_count'], range=[1000,9000], bins=8, rwidth=0.8, alpha=0.5, label='englisch', cumulative=True, density=True)\n",
    "plt.xlabel('Kapitellänge / Zeichen')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.title('Kumulative Verteilung der Kapitellängen im Markusevangelium')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anfang](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
